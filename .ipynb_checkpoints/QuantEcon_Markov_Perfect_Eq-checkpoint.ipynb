{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friday, October 16 2015, Minneapolis Fed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Perfect Equilibrium\n",
    "\n",
    "QuantEcon https://python.quantecon.org/markov_perf.html and Chapter 7 (Section 7.6 and 7.7) of Ljungqvist and Sargent (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an economy with:\n",
    "- 2 agents with utility $u_i(a_i, a_j, x)$ where $a_i$ denotes the action of $i$ and $x$ is the state variable.\n",
    "- the future is discounted at rate $\\beta$ so each agent, taking the strategy profile $a_{j}$ as given, solves:\n",
    "\\begin{align*}\n",
    "\\max_{(a_{it})_{t=0}^\\infty} &\\ \\sum_{t=0}^\\infty \\beta^t \\ u_i(a_i, a_j(h_t), x_t)\\\\\n",
    " & h_{t+1} = (h_t, (a_{it}, a_{jt}(h_t)))\\\\\n",
    " & x_{t+1} = f(a_{it}, a_{jt}(h_t), x_t)\n",
    "\\end{align*}\n",
    "Note that each player takes into account the effect that her action has on the history and hence on the action of the other player but a player takes the strategy (not the sequence of actions) of the other player as given and unaffected by her choice of srategies.\n",
    "- an action profile is Markovian if for every $i$, $a_i(x_t, h_{t-1}) = a_i(x_t, \\tilde{h}_{t-1})$ for all histories $\\tilde{h}_{t-1}$.\n",
    "- we focus on Markov Perfect Equilibria. The Markovian best response of an agent can be formulated in recursive form:\n",
    "\\begin{align*}\n",
    "v_i(x) = \\max_{a_{i}} & \\ u_i(a_i, a_j(x), x) + \\beta v_i(x')\\\\\n",
    " & x' = f(a_i, a_j(x), x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Perfect Equilibrium is a pair of value functions and Markovian best responses $(v_i, a_i)_{i=1,2}$ such that $v_i$ solves the functional equation for agent $i$ and $a_i$ is the associated policy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above problems are difficult to solve because they involve interrelated value functions iterations.\n",
    "The FOC are, for all $x$:\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial u_i(a_i, a_j(x), x)}{\\partial a_i} + \\beta v_i'(f(a_1, a_2, x)) \\frac{\\partial f(a_i, a_j(x), x)}{\\partial a_i} = 0\n",
    "\\end{equation*}\n",
    "and by Benveniste Scheickman (NEED TO CHECK),\n",
    "\\begin{equation*}\n",
    "v_i'(x') = \\frac{\\partial u_i(a_i, a_j(x'), x')}{\\partial a_j} \\frac{\\partial a_j}{\\partial x} + \\frac{\\partial u_i(a_i, a_j(x'), x')}{\\partial x} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Quadratic case:\n",
    "- notation, the choice variables are denoted by $u$ instead of $a$ \n",
    "- the objective is quadratic in $u_1, u_2, x$ with possibly an interaction term: $- \\left(x^T R_i x + u_i^T Q_i u_i + u_j^T S_i u_j\\right)$\n",
    "- the transition function is linear $x_{t+1} = A x_t + B_1 u_{1t} + B_2 u_{2t}$.\n",
    "- the value functions are quadratic $v_i = x^T P_i x$ and the policy functions are linear $a_i(x) = - F_i x$.\n",
    "- the value function iteration for each $i$ takes the form of a Riccati equations, which now involve $F_j$. (Recall ,$i$ takes into account the effect of his action on the action that $j$ will take according to $F_j$ but takes $F_j$ as given.)\n",
    "- the policy functions are the solution of a system (involving both $F_1$ and $F_2$) of linear equations.\n",
    "\n",
    "Solution:\n",
    "This problem can be solved by the method of undetermined coefficients:\n",
    "- guess that $v_i$ is quadratic: $v_i(x) = x^T P_i x$ and the policy function is linear $u_i = -F_i x$.\n",
    "- subsitute our guess for $v_i$ and $F_j$ in the functional equation for $i$.\n",
    "The following Functional Equation obtains:\n",
    "\\begin{equation*}\n",
    "x^T P_i x = \\max_{u_i} - \\bigg(x^T (R_i + F_j^T S_1 F_j) x + u_i^T Q_i u_i ) + \\beta ((A - B_jF_j)x + B_i u_i)^T P_i ((A - B_jF_j)x + B_i u_i)\\bigg)\n",
    "\\end{equation*}\n",
    "- take FOC w.r.t $u_i$ to get the $P_i$-greedy (linear) policy function, given $F_j$.\n",
    "Note that the above Functional equation has the same form as the one for an LQ dynamic programming problem except that $R$ is now $(R_i + F_j^T S_i F_j)$ and $A$ is now $(A - B_jF_j)$. So FOC give:\n",
    "$u_i = -F_i x$ where \n",
    "\\begin{equation*}\n",
    "F_i = (Q_i + \\beta B_i^T P_i B_i)^{-1} \\beta (B_i^T P_i (A - B_jF_j))\n",
    "\\end{equation*}\n",
    "- FOCs for $i=1,2$ give a system of linear equations, the solution to which is $F_1, F_2$\n",
    "\\begin{equation*}\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "F_1 &= (Q_1 +\\beta B_1^T P_1 B_1)^{-1} \\beta (B_1^T P_1 (A - B_2F_2))\\\\\n",
    "F_2 &= (Q_2 +\\beta B_2^T P_2 B_2)^{-1} \\beta (B_2^T P_2 (A - B_1F_1))\n",
    "\\end{array}\\right.\n",
    "\\end{equation*}\n",
    "- substitute the linear policy function $F_i$ in the functional equation for $i$ to get rid of the max; in the LQ case, the resulting equation is an Algebraic Riccati equation. Note the Algebraic Riccati equation for $P_i$ still depends on $F_j$.\n",
    "\\begin{equation*}\n",
    "P_i = (R_i + F_j^T S_i F_j) − (\\beta B_i^T P_i (A - B_jF_j))^T(Q_i + \\beta B_i^T P_i B_i)^{−1} (\\beta B_i^T P_i (A - B_jF_j))+ \\beta (A - B_jF_j)^T P_i (A - B_jF_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the finite horizon case:\n",
    "- first determine the final conditions $R_{i,f}$ such that payoff at $T$ is $x^T R_{i,f} x$ ($R_{i,f}$ is either given or solve the static simultaneous move game at the end)\n",
    "- compute $F_{1,T-1}, F_{2,T-1}$ by solving the system of linear equations given by FOCs computed above, taking $R_{1,f}, R_{2,f}$ as the future value function $R_f := P_T$.\n",
    "- compute $P_{i,T-1}$ by plugging $R_{i,f}$ as $P_T$ \\emph{and} F_{j,T-1} in the Algebraic Riccati equation for $i$ (note, $F_{i,T-1}$ is already substituted, we did it to get rid of the max and obtain the Algebraic Riccati equation).\n",
    "- continue by induction: first compute $F_{1,t-1}, F_{2,t-1}$ by solving the system of equations, using $P_{1,t}, P_{2,t}$. Then compute the value functions $P_{i,t-1}$ by plugging $P_{i,t}$ and $F_{j,t-1}$ in the Algebraic Riccati equation.\n",
    "\n",
    "For the infinite horizon case:\n",
    "-  take the limit of the finite horizon solution, hoping that it converges. The only difference is that the initial guess $P_i^0$ need not be $R_{i,f}$; it may be a good guess though. \n",
    "\n",
    "Note: the infinite horizon case is computed as the limit of the finite horion case but here, it is necessary to compute the policy function at every iteration to be able to compute the Riccati equation. An alternative could be to note compute the policy function every time to avoid solving the system of linear equations too much and compute the fixed point in $P_1, P_2$ for every step (this is very similar, or it may exactly be (TO CHECK) Howard's improvement algorithm); the new policy functions are computed only once the fixed point for the $P^{n-1}$-greedy policy functions is computed.\n",
    "\n",
    "\n",
    "FALSE, THIS IS FOR LQ NON SINGLE AGENT\n",
    "The solution is:\n",
    "- policy function $u_i = -F_i x$ where $F_1,F_2$ solve the linear system of equations.\n",
    "- value function $v_i(x) = x^T P_i x$ where $P_i$ solves its Algebraic Riccati Equation with the right policy function $F_j$\n",
    "- the state evolves according to:\n",
    "$x_{t+1} = (A - B_1 F_1 -  B_2 F_2) x_t$\n",
    "\n",
    "\n",
    "To do: \n",
    "- Do the case with shocks. Does Certainty Equivalence still hold.\n",
    "- Do the case with interaction terms between $u_1, u_2$ and $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Note that the functional equation for each player has the same form as the one for an LQ dynamic programming problem except that\n",
    "- $R$ is now $(R_i + F_j^T S_i F_j)$\n",
    "- $A$ is now $(A - B_jF_j)$\n",
    "- $Q$ is now $Q_i$\n",
    "- $B$ is now $B_i$\n",
    "- $P$ is now $P_i$\n",
    "- $N = C = \\epsilon = 0$\n",
    "We can therefore use the functions that were used to solve LQ problem with the appropriate substitutions. The algorithm will now need, even for the infinite horizon case, to calculate $P^n$-greedy policy functions along the sequence to update $(R_i + F_j^T S_i F_j)$ and $(A - B_jF_j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from __future__ import division\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import dot\n",
    "\n",
    "'''\n",
    "Functions to compute Markov Perfect Equilibrium MPE of a Linear Quadratic environment\n",
    "'''\n",
    "def MPE_Fpolicy(beta, P_1, P_2, A, B_1, B_2, Q_1, Q_2):\n",
    "    '''\n",
    "    Returns the matrices F_1 and F_2 which characterize the P_1,P_2-greedy policy.\n",
    "    It requires solving a system of FOCs, where each i takes F_j as given.\n",
    "    Code that gives the result: F_1, F_2 = Markov_Fpolicy(...\n",
    "    beta: discount factor\n",
    "    P_i: value function v_i(x) = x^T P_i x\n",
    "    A : effect of today's state on tomorrow's state\n",
    "    B_i: effect of today's choice by i on tomorrow's state\n",
    "    Q_i: direct effect of choice on payoff\n",
    "    '''\n",
    "    M_11 = Q_1 + beta * dot(B_1.T, dot(P_1, B_1))\n",
    "    M_12 = beta * dot(B_1.T, dot(P_1, B_2))\n",
    "    M_21 = beta * dot(B_2.T, dot(P_2, B_1))\n",
    "    M_22 = Q_2 + beta * dot(B_2.T, dot(P_2, B_2)) \n",
    "    v_1 = beta * dot(B_1.T, dot(P_1, A))\n",
    "    v_2 = beta * dot(B_2.T, dot(P_2, A))\n",
    "    M = np.array([[M_11, M_12], [M_21, M_22]])\n",
    "    M.shape = (k_1 + k_2, k_1 + k_2)\n",
    "    v = np.array([v_1, v_2])\n",
    "    v.shape = (k_1 + k_2, n)\n",
    "    Sol = np.linalg.solve(M, v)\n",
    "    return Sol[0:k_1], Sol[k_1: k_1+k_2]\n",
    "\n",
    "def LQpolicy(F, x):\n",
    "    '''\n",
    "    Choice is u_i = - F_i x\n",
    "    x : state variable(s)\n",
    "    '''\n",
    "    return - dot(F,x)\n",
    "\n",
    "def LQvalue(x, P, beta, C):\n",
    "    '''\n",
    "    Value function: v_i(x) = x^T P_i x + d\n",
    "    d accounts for the effect of risk on the value function\n",
    "    '''\n",
    "    d = (beta /(1 - beta)) * np.trace(dot(C.T, dot(P,C)))\n",
    "    return dot(x.T, dot(P,x)) + d\n",
    "\n",
    "def next_state_MPE(x, A, B_1, B_2, F_1, F_2, epsilon):\n",
    "    '''\n",
    "    Computes the next state according to x_{t+1} = A x_t + B_1 u_1 + B_2 u_2\n",
    "    A: effect of today's state on tomorrow's state\n",
    "    B_i: effect of today's choice by i on tomorrow's state\n",
    "    F_i: Markovian best response is u_i = - F_i x\n",
    "    '''\n",
    "    return dot((A - dot(B_1,F_1) - dot(B_2,F_2)),x)\n",
    "\n",
    "def update_Riccati_MPE_Fj(P, beta, A, B, R, Q, S, B_j, F_j):\n",
    "    '''\n",
    "    Updates the value function for a Linear Quadratic environment\n",
    "    x^T P_i x = \\max_{u_i} - \\bigg(x^T (R_i + F_j^T S_1 F_j) x + u_i^T Q_i u_i ) + \\beta ((A - B_jF_j)x + B_i u_i)^T P_i ((A - B_jF_j)x + B_i u_i)\\bigg)\n",
    "    P: inital value function\n",
    "    A: effect of today's state on tomorrow's state\n",
    "    B: effect of today's choice by i on tomorrow's state\n",
    "    R: effect of state on payoff\n",
    "    Q: direct effect of choice on payoff\n",
    "    S: direct effect of other's choice on payoff (externality)\n",
    "    B_j: effect of other's choice on tomorrow's state\n",
    "    F_j: choice function u_j = - F_j x of the other, taken as given.\n",
    "    '''\n",
    "    New_R = R + dot(F_j.T, dot(S, F_j))\n",
    "    New_A = A - dot(B_j, F_j)\n",
    "    New_Z = (beta * dot(B.T, dot(P, New_A)))\n",
    "    W = inv(Q + beta * dot(B.T, dot(P,B)))\n",
    "    return New_R - dot(New_Z.T, dot(W, New_Z)) + beta * dot(New_A.T, dot(P, New_A))\n",
    "\n",
    "def Riccati_MPE_F_j(init_P, beta, A, B, R, Q, S, B_j, F_j):\n",
    "    #this could be useful to do something similar to Howard's improvement algorithm\n",
    "    #the system of simultaneous equations would be solved less often and more Riccati iterations would be done.\n",
    "    #should include a maximum number of iterations.\n",
    "    dist = 1\n",
    "    while dist > 10e-3:\n",
    "        next_P = update_Riccati_MPE(init_P, beta, A, B, R, Q, S, B_j, F_j)\n",
    "        dist = np.linalg.norm(init_P - next_P)\n",
    "        init_P = next_P\n",
    "    return next_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LQMarkovPerfect(max_iter, tol, init_P_1, init_P_2, beta, A, B_1, B_2, R_1, R_2, Q_1, Q_2, S_1, S_2):\n",
    "    '''\n",
    "    This is the main loop to compute Markov Perfect Equilibrium in a Linear Quadratic environment:\n",
    "        - F_1, F_2 for Markovian Perfect Equilibrium functions u_i = - F_i x_i\n",
    "        - P_1, P_2 for value functions of the Markov Perfect Equilibrium, v_i(x) = x^T P_i x\n",
    "    LQ environment: x^T P_i x = \\max_{u_i} - \\bigg(x^T (R_i + F_j^T S_1 F_j) x + u_i^T Q_i u_i ) + \\beta ((A - B_jF_j)x + B_i u_i)^T P_i ((A - B_jF_j)x + B_i u_i)\\bigg)\n",
    "    max_iter: stop if this number of iterations is reached\n",
    "    tol: tolerance below which the sequence is near enough convergence\n",
    "    init_P_i: initial guess of value function\n",
    "    beta: discount factor\n",
    "    A : effect of today's state on tomorrow's state\n",
    "    B_i: effect of today's choice by i on tomorrow's state\n",
    "    R_i: effect of state on payoff\n",
    "    Q_i: direct effect of choice on payoff\n",
    "    S_i: direct effect of other's choice on payoff (externality)\n",
    "    '''\n",
    "    for it in range(max_iter):\n",
    "        # compute the init_P-greedy policy functions which will be fed into this round of value function iteration\n",
    "        F_1, F_2 = MPE_Fpolicy(beta, init_P_1, init_P_2, A, B_1, B_2, Q_1, Q_2)\n",
    "        F_1.shape = (k_1, n)\n",
    "        F_2.shape = (k_2, n)\n",
    "        # compute the next element of the sequence of value function on the way to convergence\n",
    "        P_1 = update_Riccati_MPE_Fj(init_P_1, beta, A, B_1, R_1, Q_1, S_1, B_2, F_2)\n",
    "        P_2 = update_Riccati_MPE_Fj(init_P_2, beta, A, B_2, R_2, Q_2, S_2, B_1, F_1)        \n",
    "        #distance between the last two elements in the sequence\n",
    "        dist_1 = np.linalg.norm(init_P_1 - P_1)\n",
    "        dist_2 = np.linalg.norm(init_P_2 - P_2)\n",
    "        #update and loop back unless the Cauchy sequence has converged\n",
    "        init_P_1 = P_1\n",
    "        init_P_2 = P_2\n",
    "        if max(dist_1, dist_2) < tol:\n",
    "            print(it, 'number of iterations')\n",
    "            break\n",
    "            return F_1, F_2, P_1, P_2, 'F_1, F_2, P_1, P_2'\n",
    "    else:\n",
    "        print('not converged')\n",
    "    #compute the policy functions associated with the last value functions in from the loop.\n",
    "    F_1, F_2 = MPE_Fpolicy(beta, P_1, P_2, A, B_1, B_2, Q_1, Q_2)\n",
    "    return F_1, F_2, P_1, P_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 number of iterations\n",
      "[[-0.66846613  0.29512482  0.07584666]]\n",
      "[[-0.66846613  0.07584666  0.29512482]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'F_2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Environment\n",
    "'''\n",
    "# Parameters\n",
    "gamma = 12\n",
    "a_0 = 10\n",
    "a_1 = 2\n",
    "beta = 0.96\n",
    "#c = 2 \n",
    "#rho = 0.9\n",
    "#sigma = 0.15\n",
    "\n",
    "# Elements of the LQ environment\n",
    "n = 3 #number of state variables\n",
    "k_1 = 1 #number of control variables for player 1\n",
    "k_2 = 1 #number of control variables for player 2\n",
    "R_1 = np.array([[0, -0.5 * a_0, 0],\n",
    "                [-0.5 * a_0, a_1, 0.5 * a_1],\n",
    "                [0, 0.5 * a_1, 0]])\n",
    "R_2 = np.array([[0, 0, -0.5 * a_0],\n",
    "                [0, 0, 0.5 * a_1],\n",
    "                [-0.5 * a_0, 0.5 * a_1, a_1]])\n",
    "Q_1 = np.array([gamma])\n",
    "Q_2 = np.array([gamma])\n",
    "S_1 = np.array([[0]])\n",
    "S_2 = np.array([[0]])\n",
    "A = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [0, 0, 1]])\n",
    "B_1 = np.array([[0], [1], [0]])\n",
    "B_2 = np.array([[0], [0], [1]])\n",
    "\n",
    " \n",
    "'''\n",
    "Main loop\n",
    "'''\n",
    "max_iter = 1000 # maximum number of iterations\n",
    "tol  = 10e-9 # tolerance level below which convergence is close enough\n",
    "init_P_1 = np.eye(3) # initial guess of value function\n",
    "init_P_2 = np.eye(3)\n",
    "\n",
    "F_1, F_2, P_1, P_2 = LQMarkovPerfect(max_iter, tol, init_P_1, init_P_2, beta, A, B_1, B_2, R_1, R_2, Q_1, Q_2, S_1, S_2)\n",
    "print(F_1), 'F_1'\n",
    "print(F_2), 'F_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution coincides with the code from QuantEcon https://python.quantecon.org/markov_perf.html#Background\n",
    "\\begin{align*}\n",
    "F1 &= [[-0.66846615, \\quad   0.29512482, \\quad   0.07584666]]\\\\\n",
    "F2 &= [[-0.66846615, \\quad   0.07584666, \\quad   0.29512482]]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
